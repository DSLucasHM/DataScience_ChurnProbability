{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with Neural Networks\n",
    "\n",
    "This notebook implements a neural network approach to customer churn prediction. It includes data preprocessing, model training with cross-validation, evaluation, and prediction capabilities.\n",
    "\n",
    "Author: Lucas Miyazawa (Improved by Claude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers, optimizers, metrics\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(df, target_col='Churn', test_size=0.25, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataframe into training and validation sets while maintaining class distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to split\n",
    "    target_col : str, default='Churn'\n",
    "        Name of the target column\n",
    "    test_size : float, default=0.25\n",
    "        Proportion of dataset to include in validation split (0.25 = 25%)\n",
    "    random_state : int, default=42\n",
    "        Controls the shuffling for reproducible output\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple:\n",
    "        train_df, valid_df - Split dataframes\n",
    "    \"\"\"\n",
    "    # Perform stratified split to maintain class distribution\n",
    "    train_df, valid_df = train_test_split(\n",
    "        df, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=df[target_col]\n",
    "    )\n",
    "    \n",
    "    # Print split information\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Training set shape: {train_df.shape} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Validation set shape: {valid_df.shape} ({len(valid_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"\\nClass distribution in training set:\\n{train_df[target_col].value_counts(normalize=True)}\")\n",
    "    print(f\"\\nClass distribution in validation set:\\n{valid_df[target_col].value_counts(normalize=True)}\")\n",
    "    \n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline(df, target_col='Churn', id_cols=None):\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline for customer data that handles both\n",
    "    numerical and categorical features appropriately.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset to preprocess\n",
    "    target_col : str, default='Churn'\n",
    "        The name of the target column\n",
    "    id_cols : list or str, default=None\n",
    "        Column(s) containing IDs to exclude from preprocessing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple:\n",
    "        - preprocessor: ColumnTransformer object fit to the data\n",
    "        - input_dim: Integer representing the dimensionality after preprocessing\n",
    "    \"\"\"\n",
    "    # Initialize default value for id_cols if None\n",
    "    if id_cols is None:\n",
    "        id_cols = ['CustomerID']\n",
    "    \n",
    "    # Ensure id_cols is a list\n",
    "    if isinstance(id_cols, str):\n",
    "        id_cols = [id_cols]\n",
    "\n",
    "    # Define columns to exclude from processing\n",
    "    exclude_cols = [target_col] + id_cols if target_col else id_cols\n",
    "    \n",
    "    # Separate numerical and categorical columns\n",
    "    num_columns = []\n",
    "    cat_columns = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in exclude_cols:\n",
    "            continue\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            num_columns.append(col)\n",
    "        else:\n",
    "            cat_columns.append(col)\n",
    "\n",
    "    # Build transformation pipelines\n",
    "    transformers = []\n",
    "    input_dim = 0\n",
    "\n",
    "    # Add numerical transformer if numerical columns exist\n",
    "    if num_columns:\n",
    "        num_transformer = StandardScaler()\n",
    "        transformers.append(('num', num_transformer, num_columns))\n",
    "        input_dim += len(num_columns)\n",
    "\n",
    "    # Add categorical transformers for each categorical column\n",
    "    for col in cat_columns:\n",
    "        transformer = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        transformers.append((f'cat_{col}', transformer, [col]))\n",
    "        input_dim += df[col].nunique()\n",
    "\n",
    "    # Create and fit the preprocessor\n",
    "    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "    train_features = df.drop(columns=exclude_cols, errors='ignore').copy()\n",
    "    preprocessor.fit(train_features)\n",
    "\n",
    "    return preprocessor, input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, preprocessor=None, target_col='Churn', id_cols=None, shuffle=True, batch_size=256):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a TensorFlow Dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The dataset to convert\n",
    "    preprocessor : ColumnTransformer, default=None\n",
    "        Preprocessor to transform the features\n",
    "    target_col : str, default='Churn'\n",
    "        Name of the target column\n",
    "    id_cols : list or str, default=None\n",
    "        Column(s) containing IDs to exclude\n",
    "    shuffle : bool, default=True\n",
    "        Whether to shuffle the dataset\n",
    "    batch_size : int, default=256\n",
    "        Batch size for the dataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.data.Dataset\n",
    "        A TensorFlow dataset ready for model training\n",
    "    \"\"\"\n",
    "    # Initialize default value for id_cols if None\n",
    "    if id_cols is None:\n",
    "        id_cols = ['CustomerID']\n",
    "    \n",
    "    # Ensure id_cols is a list\n",
    "    if isinstance(id_cols, str):\n",
    "        id_cols = [id_cols]\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # Extract labels\n",
    "    labels = df.pop(target_col)\n",
    "    \n",
    "    # Remove ID columns\n",
    "    for col in id_cols:\n",
    "        if col in df.columns:\n",
    "            df.pop(col)\n",
    "    \n",
    "    # Apply preprocessing if provided\n",
    "    if preprocessor is not None:\n",
    "        features = preprocessor.transform(df).astype(np.float32)\n",
    "    else:\n",
    "        features = df.values.astype(np.float32)\n",
    "    \n",
    "    # Convert labels to appropriate format\n",
    "    labels = labels.values.astype(np.int32)\n",
    "    \n",
    "    # Create the dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    \n",
    "    # Apply batching and prefetching for performance\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, dropout_rate=0.7, l2_reg=0.15, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    Create a neural network model for churn prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Dimensionality of the input features\n",
    "    dropout_rate : float, default=0.7\n",
    "        Dropout rate for regularization\n",
    "    l2_reg : float, default=0.15\n",
    "        L2 regularization strength\n",
    "    learning_rate : float, default=0.0001\n",
    "        Learning rate for the Adam optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.keras.Model\n",
    "        Compiled neural network model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(shape=(input_dim,)),\n",
    "        layers.Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, target_col='Churn', id_cols=None, n_splits=4, batch_size=64, \n",
    "                epochs=5, verbose=0, model_params=None):\n",
    "    \"\"\"\n",
    "    Train a neural network model using cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset for training\n",
    "    target_col : str, default='Churn'\n",
    "        Name of the target column\n",
    "    id_cols : list or str, default=None\n",
    "        Column(s) containing IDs to exclude\n",
    "    n_splits : int, default=3\n",
    "        Number of cross-validation folds\n",
    "    batch_size : int, default=64\n",
    "        Batch size for training\n",
    "    epochs : int, default=100\n",
    "        Maximum number of training epochs\n",
    "    verbose : int, default=0\n",
    "        Verbosity mode (0, 1, or 2)\n",
    "    model_params : dict, default=None\n",
    "        Parameters for model creation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the best model, preprocessor, scores, and fold information\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    if id_cols is None:\n",
    "        id_cols = ['CustomerID']\n",
    "    \n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'dropout_rate': 0.7,\n",
    "            'l2_reg': 0.15,\n",
    "            'learning_rate': 0.0001\n",
    "        }\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_auc = 0\n",
    "    best_model = None\n",
    "    best_preprocessor = None\n",
    "    best_fold = 0\n",
    "    auc_scores = []\n",
    "\n",
    "    # Loop through each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[target_col]), 1):\n",
    "        print(f\"\\nFold {fold}/{n_splits}\")\n",
    "\n",
    "        # Split data for this fold\n",
    "        train_df = df.iloc[train_idx]\n",
    "        val_df = df.iloc[val_idx]\n",
    "\n",
    "        # Create preprocessing pipeline\n",
    "        preprocessor, input_dim = create_preprocessing_pipeline(\n",
    "            train_df,\n",
    "            target_col=target_col,\n",
    "            id_cols=id_cols\n",
    "        )\n",
    "\n",
    "        # Convert to TensorFlow datasets\n",
    "        train_ds = df_to_dataset(train_df, preprocessor, target_col, id_cols, shuffle=True, batch_size=batch_size)\n",
    "        val_ds = df_to_dataset(val_df, preprocessor, target_col, id_cols, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        # Create and compile model\n",
    "        model = create_model(\n",
    "            input_dim=input_dim,\n",
    "            dropout_rate=model_params.get('dropout_rate', 0.7),\n",
    "            l2_reg=model_params.get('l2_reg', 0.15),\n",
    "            learning_rate=model_params.get('learning_rate', 0.0001)\n",
    "        )\n",
    "\n",
    "        # Set up early stopping\n",
    "        cb_early = callbacks.EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=5,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(\n",
    "            train_ds, \n",
    "            validation_data=val_ds, \n",
    "            epochs=epochs, \n",
    "            callbacks=[cb_early], \n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        val_metrics = model.evaluate(val_ds, verbose=0)\n",
    "        auc = val_metrics[1]\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "        # Track best model\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_model = model\n",
    "            best_preprocessor = preprocessor\n",
    "            best_fold = fold\n",
    "\n",
    "    # Report results\n",
    "    print(\"\\nFold results:\")\n",
    "    for i, score in enumerate(auc_scores, 1):\n",
    "        print(f\"Fold {i}: AUC = {score:.4f}\")\n",
    "    print(f\"\\nAverage AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "    print(f\"Best model was from fold {best_fold} with AUC = {best_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'preprocessor': best_preprocessor,\n",
    "        'auc': best_auc,\n",
    "        'fold': best_fold,\n",
    "        'all_scores': auc_scores,\n",
    "        'input_dim': input_dim\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, preprocessor, valid_df, target_col='Churn', id_cols=None, optimize_threshold=True):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : tf.keras.Model\n",
    "        Trained model to evaluate\n",
    "    preprocessor : ColumnTransformer\n",
    "        Preprocessor for feature transformation\n",
    "    valid_df : pandas.DataFrame\n",
    "        Validation dataset\n",
    "    target_col : str, default='Churn'\n",
    "        Name of the target column\n",
    "    id_cols : list or str, default=None\n",
    "        Column(s) containing IDs to exclude\n",
    "    optimize_threshold : bool, default=True\n",
    "        Whether to optimize the classification threshold\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing evaluation metrics and threshold\n",
    "    \"\"\"\n",
    "    # Initialize default value for id_cols if None\n",
    "    if id_cols is None:\n",
    "        id_cols = ['CustomerID']\n",
    "        \n",
    "    # Ensure id_cols is a list\n",
    "    if isinstance(id_cols, str):\n",
    "        id_cols = [id_cols]\n",
    "    \n",
    "    # Clean data\n",
    "    df = valid_df.copy()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_new = df.drop(columns=[target_col] + id_cols, errors='ignore')\n",
    "    y_true = df[target_col].values\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_new_transformed = preprocessor.transform(X_new).astype(np.float32)\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_probs = model.predict(X_new_transformed, verbose=1).flatten()\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc_score = roc_auc_score(y_true, pred_probs)\n",
    "    \n",
    "    # Find optimal threshold if requested\n",
    "    threshold = 0.5\n",
    "    if optimize_threshold:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, pred_probs)\n",
    "        # Find threshold that maximizes the difference between TPR and FPR\n",
    "        threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "    \n",
    "    # Apply threshold to get binary predictions\n",
    "    pred_labels = (pred_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, pred_labels)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "    print(f\"Optimal threshold: {threshold:.4f}\")\n",
    "    print(cm)\n",
    "    \n",
    "    return {\n",
    "        'auc': auc_score,\n",
    "        'threshold': threshold,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_true': y_true,\n",
    "        'pred_probs': pred_probs,\n",
    "        'pred_labels': pred_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Deployment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, preprocessor, data_df, id_col='CustomerID', \n",
    "                         target_col='Churn', threshold=0.5, predictions_dir = 'Predictions', output_file='churn_predictions.csv'):\n",
    "    \"\"\"\n",
    "    Generate and save predictions for a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : tf.keras.Model\n",
    "        Trained model to use for predictions\n",
    "    preprocessor : ColumnTransformer\n",
    "        Preprocessor for feature transformation\n",
    "    data_df : pandas.DataFrame\n",
    "        Dataset to predict on\n",
    "    id_col : str, default='CustomerID'\n",
    "        Name of the ID column\n",
    "    target_col : str, default='Churn'\n",
    "        Name of the target column (if exists)\n",
    "    threshold : float, default=0.5\n",
    "        Classification threshold\n",
    "    output_file : str, default='churn_predictions.csv'\n",
    "        File path to save the predictions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing predictions\n",
    "    \"\"\"\n",
    "    # Clean data\n",
    "    df = data_df.copy()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Store IDs\n",
    "    customer_ids = df[id_col].values\n",
    "    \n",
    "    # Prepare features, excluding target if it exists\n",
    "    columns_to_drop = [id_col]\n",
    "    if target_col in df.columns:\n",
    "        columns_to_drop.append(target_col)\n",
    "    \n",
    "    X_new = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_new_transformed = preprocessor.transform(X_new).astype(np.float32)\n",
    "    \n",
    "    # Generate predictions\n",
    "    pred_probs = model.predict(X_new_transformed, verbose=1).flatten()\n",
    "    pred_labels = (pred_probs > threshold).astype(int)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        'customer_id': customer_ids,\n",
    "        'churn_probability': pred_probs,\n",
    "        'churn_prediction': pred_labels\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Save to CSV\n",
    "    os.makedirs(predictions_dir, exist_ok=True)\n",
    "    predictions_path = os.path.join(predictions_dir, output_file)\n",
    "    output_df.to_csv(predictions_path, index=False)\n",
    "    \n",
    "    print(f\"File '{predictions_path}' saved successfully!\")\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, preprocessor, model_dir=\"Models\", model_name=\"model_churn_tf\"):\n",
    "    \"\"\"\n",
    "    Save the model and preprocessor for future use.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : tf.keras.Model\n",
    "        Trained model to save\n",
    "    preprocessor : ColumnTransformer\n",
    "        Fitted preprocessor to save\n",
    "    model_dir : str, default=\"Models\"\n",
    "        Directory to save the model artifacts\n",
    "    model_name : str, default=\"model_churn_tf\"\n",
    "        Name for the saved model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with paths to saved files\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(model_dir , exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(model_dir, model_name + \".keras\")\n",
    "    model.save(model_path)\n",
    "    \n",
    "    # Save preprocessor\n",
    "    preprocessor_path = os.path.join(model_dir, \"preprocessor.pkl\")\n",
    "    with open(preprocessor_path, \"wb\") as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "    \n",
    "    print(f\"✅ Model saved to '{model_path}'\")\n",
    "    print(f\"✅ Preprocessor saved to '{preprocessor_path}'\")\n",
    "    \n",
    "    return {\n",
    "        'model_path': model_path,\n",
    "        'preprocessor_path': preprocessor_path\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_dir=\"Models\", model_name=\"model_churn_tf\"):\n",
    "    \"\"\"\n",
    "    Load a saved model and preprocessor.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_dir : str, default=\"Models\"\n",
    "        Directory where model artifacts are stored\n",
    "    model_name : str, default=\"model_churn_tf\"\n",
    "        Name of the saved model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Loaded model and preprocessor\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load preprocessor\n",
    "    preprocessor_path = os.path.join(model_dir, \"preprocessor.pkl\")\n",
    "    preprocessor = joblib.load(preprocessor_path)\n",
    "    \n",
    "    print(f\"✅ Model loaded from '{model_path}'\")\n",
    "    print(f\"✅ Preprocessor loaded from '{preprocessor_path}'\")\n",
    "    \n",
    "    return model, preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Splitting data into training and validation sets...\n",
      "Original dataset shape: (440832, 12)\n",
      "Training set shape: (330624, 12) (75.0%)\n",
      "Validation set shape: (110208, 12) (25.0%)\n",
      "\n",
      "Class distribution in training set:\n",
      "Churn\n",
      "1.0    0.567106\n",
      "0.0    0.432894\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in validation set:\n",
      "Churn\n",
      "1.0    0.567109\n",
      "0.0    0.432891\n",
      "Name: proportion, dtype: float64\n",
      "Training model...\n",
      "\n",
      "Fold 1/4\n",
      "\n",
      "Fold 2/4\n",
      "\n",
      "Fold 3/4\n",
      "\n",
      "Fold 4/4\n",
      "\n",
      "Fold results:\n",
      "Fold 1: AUC = 0.9547\n",
      "Fold 2: AUC = 0.9551\n",
      "Fold 3: AUC = 0.9553\n",
      "Fold 4: AUC = 0.9555\n",
      "\n",
      "Average AUC: 0.9552 ± 0.0003\n",
      "Best model was from fold 4 with AUC = 0.9555\n",
      "Evaluating model on validation set...\n",
      "\u001b[1m3444/3444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 802us/step\n",
      "AUC: 0.9548\n",
      "Optimal threshold: 0.5567\n",
      "[[43966  3742]\n",
      " [ 8843 53657]]\n",
      "Saving model...\n",
      "✅ Model saved to 'Models\\model_churn_tf.keras'\n",
      "✅ Preprocessor saved to 'Models\\preprocessor.pkl'\n",
      "Generating predictions on validation set...\n",
      "\u001b[1m3444/3444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 793us/step\n",
      "File 'Predictions\\churn_predictions.csv' saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment and run the below code when you have your data ready\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv('Dataset/customer_data.csv')  # Replace with your dataset\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    print(\"Splitting data into training and validation sets...\")\n",
    "    train_df, valid_df = split_train_valid(df, test_size=0.25)\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    results = train_model(train_df)\n",
    "    best_model = results['model']\n",
    "    best_preprocessor = results['preprocessor']\n",
    "    \n",
    "    print(\"Evaluating model on validation set...\")\n",
    "    eval_results = evaluate_model(best_model, best_preprocessor, valid_df)\n",
    "    \n",
    "    print(\"Saving model...\")\n",
    "    save_model(best_model, best_preprocessor)\n",
    "    \n",
    "    print(\"Generating predictions on validation set...\")\n",
    "    pred_df = generate_predictions(best_model, best_preprocessor, valid_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
